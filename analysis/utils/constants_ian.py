DATA_NAME_LATEX = {
    "dolma17": r"\dolmaSeventeen{}",
    "no_code": r"\noCode{}",
    "no_math_no_code": r"\noMathNoCode{}",
    "no_reddit": r"\noReddit{}",
    "no_flan": r"\noFlan{}",
    "dolma-v1-6-and-sources-baseline": r"\dolmaVSixteenAndSourcesBaseline{}",
    "c4": r"\cFour{}",
    "prox_fineweb_pro": r"\proxFinewebPro{}",
    "fineweb_edu_dedup": r"\finewebEduDedup{}",
    "falcon": r"\falcon{}",
    "falcon_and_cc": r"\falconAndCc{}",
    "falcon_and_cc_eli5_oh_top10p": r"\falconAndCcEliFiveOhTopTenP{}",
    "falcon_and_cc_eli5_oh_top20p": r"\falconAndCcEliFiveOhTopTwentyP{}",
    "falcon_and_cc_og_eli5_oh_top10p": r"\falconAndCcOgEliFiveOhTopTenP{}",
    "falcon_and_cc_tulu_qc_top10": r"\falconAndCcTuluQcTopTen{}",
    "DCLM-baseline": r"\DCLMBaseline{}",
    "dolma17-75p-DCLM-baseline-25p": r"\dolmaSeventeenSeventyFivePDCLMBaselineTwentyFiveP{}",
    "dolma17-50p-DCLM-baseline-50p": r"\dolmaSeventeenFiftyPDCLMBaselineFiftyP{}",
    "dolma17-25p-DCLM-baseline-75p": r"\dolmaSeventeenTwentyFivePDCLMBaselineSeventyFiveP{}",
    "dclm_ft7percentile_fw2": r"\dclmFtSevenPercentileFwTwo{}",
    "dclm_ft7percentile_fw3": r"\dclmFtSevenPercentileFwThree{}",
    "dclm_fw_top10": r"\dclmFwTopTen{}",
    "dclm_fw_top3": r"\dclmFwTopThree{}",
    "pos_eli5_oh_neg_dclm_refinedweb_steps_2000_lr3e4_top10p": r"\posEliFiveOhNegDclmRefinedwebStepsTwoThousandLrThreeEFourTopTenP{}",
    "pos_eli5_oh_neg_dclm_refinedweb_steps_2000_lr3e4_top20p": r"\posEliFiveOhNegDclmRefinedwebStepsTwoThousandLrThreeEFourTopTwentyP{}"
}

DATA_NAME_CLEAN = {
    "dolma17": "Dolma1.7",
    "no_code": "noCode",
    "no_math_no_code": "noMathNoCode", 
    "no_reddit": "noReddit",
    "no_flan": "noFlan",
    "dolma-v1-6-and-sources-baseline": "Dolma1.6++",
    "c4": "C4",
    "prox_fineweb_pro": "FineWeb-Pro",
    "fineweb_edu_dedup": "FineWeb-Edu",
    "falcon": "Falcon",
    "falcon_and_cc": "Falcon+CC",
    "falcon_and_cc_eli5_oh_top10p": "Eli5-Oh-Top10p",
    "falcon_and_cc_eli5_oh_top20p": "Eli5-Oh-Top20p",
    "falcon_and_cc_og_eli5_oh_top10p": "Og-Eli5-Oh-Top10p",
    "falcon_and_cc_tulu_qc_top10": "Tulu-Top10",
    "DCLM-baseline": "DCLM-Baseline",
    "dolma17-75p-DCLM-baseline-25p": "Dolma-75p-DCLM-25p",
    "dolma17-50p-DCLM-baseline-50p": "Dolma-50p-DCLM-50p",
    "dolma17-25p-DCLM-baseline-75p": "Dolma-25p-DCLM-75p",
    "dclm_ft7percentile_fw2": "dclm-ft7percentile-fw2",
    "dclm_ft7percentile_fw3": "dclm-ft7percentile-fw3",
    "dclm_fw_top10": "dclm-fw-top10",
    "dclm_fw_top3": "dclm-fw-top3",
    "pos_eli5_oh_neg_dclm_refinedweb_steps_2000_lr3e4_top10p": "dclm-Eli5-Oh-top10p",
    "pos_eli5_oh_neg_dclm_refinedweb_steps_2000_lr3e4_top20p": "dclm-Eli5-Oh-top20p"
}


SETUP_NAME_LATEX = {
    '3_param-default':                         'FLOPs $\\rightarrow$ Task Loss $\\rightarrow$ Metric (2 step, FLOPs)',
    '3_param-default-helper_points':           '  + helper point',
    '3_param-default-step2=0.5':               '  + using final 50% of checkpoints for step 2 prediction',
    '3_param-default-helper_points-step2=0.5': '  + helper point + final 50% of checkpoints',
    '2_param-default':                         '  + remove the irreducible error term $E$',
    '3_param-1_step':                          'FLOPs $\\rightarrow$ Metric (1 step)',
    '5_param-ai2':                             '$(N, D)$ $\\rightarrow$ Task Loss $\\rightarrow$ Metric (2 step, $(N, D)$)',
    '5_param-1_step-ai2':                      '$(N, D)$ $\\rightarrow$ Metric (1 step, $(N, D)$)',
    '3_param-no_750M':                         'FLOPs $\\rightarrow$ Task Loss $\\rightarrow$ Metric (2 step, FLOPs)',
    '3_param-no_750M-helper_points':           '  + helper point',
    '3_param-no_750M-step2=0.5':               '  + using final 50% of checkpoints for step 2 prediction',
    '3_param-no_750M-helper_points-step2=0.5': '  + helper point + final 50% of checkpoints',
    '2_param-no_750M':                         '  + remove the irreducible error term $E$',
    '3_param-1_step-no_750M':                  'FLOPs $\\rightarrow$ Metric (1 step)',
    '3_param-no_750M_no_530M':                 'FLOPs $\\rightarrow$ Task Loss $\\rightarrow$ Metric (2 step, FLOPs)',
    '3_param-no_750M_no_530M-helper_points':   '  + helper point',
    '3_param-no_750M_no_530M-step2=0.5':       '  + using final 50% of checkpoints for step 2 prediction',
    '3_param-no_750M_no_530M-helper_points-step2=0.5': '  + helper point + final 50% of checkpoints',
    '2_param-no_750M_no_530M':                 '  + remove the irreducible error term $E$',
    '3_param-1_step-no_750M_no_530M':          'FLOPs $\\rightarrow$ Metric (1 step)',
}

TASK_NAME_LATEX = {
    'olmes_10_macro_avg': 'OLMES Avg.',
    'mmlu': 'MMLU',
    'arc_challenge': 'ARC-Challenge',
    'arc_easy': 'ARC-Easy',
    'boolq': 'BoolQ',
    'csqa': 'CommonsenseQA',
    'hellaswag': 'HellaSwag',
    'openbookqa': 'OpenBookQA',
    'piqa': 'PIQA',
    'socialiqa': 'SocialIQA',
    'winogrande': 'WinoGrande'
}

